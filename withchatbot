import os
import fitz  # PyMuPDF
import pickle
from pathlib import Path
from flask import Flask, request, jsonify, render_template
from sentence_transformers import SentenceTransformer, util
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

app = Flask(__name__)

PDF_FOLDER = "/home/dans/workspace/smc_llms/in/pdfs"

# Load models once
print("Running global code")
sentence_model = gpt2_tokenizer = gpt2_model = None
if sentence_model is None:
    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
if gpt2_tokenizer is None:
    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
if gpt2_model is None:
    gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')

def extract_text_with_page_numbers(pdf_path):
    doc = fitz.open(pdf_path)
    pdf_text = {}
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text = page.get_text()
        pdf_text[page_num + 1] = text
    return pdf_text

def create_embeddings(model, pdf_text):
    embeddings = {}
    for page_num, text in pdf_text.items():
        sentences = text.split('\n')
        embeddings[page_num] = model.encode(sentences, convert_to_tensor=True)
    return embeddings

def load_pdfs():
    pdf_data = {}
    for pdf_file in os.listdir(PDF_FOLDER):
        if pdf_file.endswith(".pdf"):
            pdf_path = os.path.join(PDF_FOLDER, pdf_file)
            pdf_text = extract_text_with_page_numbers(pdf_path)
            pdf_embeddings = create_embeddings(sentence_model, pdf_text)
            pdf_data[pdf_file] = (pdf_text, pdf_embeddings)
    return pdf_data

cache = Path("in/pdf_emb.pkl")
print("Checking cache")
if cache.exists():
    print("Loading cache")
    pdf_data = pickle.loads(cache.read_bytes())
else:
    print("Creating PDF embeddings")
    pdf_data = load_pdfs()
    print("Storing cache")
    pickle.dump(pdf_data, cache.open("wb"))


def search_answer_in_pdfs(question):
    question_embedding = sentence_model.encode(question, convert_to_tensor=True)
    best_score = -1
    best_result = None

    for pdf_name, (pdf_text, pdf_embeddings) in pdf_data.items():
        for page_num, embeddings in pdf_embeddings.items():
            scores = util.pytorch_cos_sim(question_embedding, embeddings)
            max_score, max_idx = scores.max(dim=1)
            if max_score.item() > best_score:
                best_score = max_score.item()
                best_result = {
                    "pdf": pdf_name,
                    "page": page_num,
                    "sentence": pdf_text[page_num].split('\n')[max_idx.item()]
                }

    if best_result and best_score > 0.5:
        return f"Answer found in {best_result['pdf']} (Page {best_result['page']}): {best_result['sentence']}"
    else:
        return "No relevant answer found in the PDFs."

def generate_answer(question, context):
    input_text = f"Question: {question}\nContext: {context}\nAnswer:"
    inputs = gpt2_tokenizer.encode(input_text, return_tensors='pt')
    outputs = gpt2_model.generate(inputs, max_length=150, num_return_sequences=1)
    answer = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer.split("Answer:")[-1].strip()

@app.route('/')
def home():
    # return render_template("chat.html")
    with open("templates/chat.html", "rt") as file:
        return file.read()


@app.route('/chat', methods=['POST'])
def chat():
    data = request.json
    question = data.get('question')
    if not question:
        return jsonify({"error": "No question provided"}), 400

    retrieved_text = search_answer_in_pdfs(question)
    generated_answer = generate_answer(question, retrieved_text)
    
    return jsonify({"question": question, "answer": generated_answer})

if __name__ == "__main__":
    print("Starting server...")
    app.run(host='0.0.0.0', port=5000, debug=True)

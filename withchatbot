import os
import fitz  # PyMuPDF
import pickle
from pathlib import Path
from flask import Flask, request, jsonify
from sentence_transformers import SentenceTransformer, util
from transformers import GPT2Tokenizer, GPT2LMHeadModel

app = Flask(__name__)

PDF_FOLDER = "/home/dans/workspace/smc_llms/in/pdfs"

# Load models globally
print("Initializing models...")
sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')

def extract_text_with_page_numbers(pdf_path):
    """Extracts text from a PDF with page numbers."""
    doc = fitz.open(pdf_path)
    pdf_text = {}
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text = page.get_text("text")
        if text.strip():  # Avoid empty pages
            pdf_text[page_num + 1] = text
    return pdf_text

def create_embeddings(model, pdf_text):
    """Creates sentence embeddings for each page in a PDF."""
    embeddings = {}
    for page_num, text in pdf_text.items():
        sentences = text.split('\n')
        embeddings[page_num] = model.encode(sentences, convert_to_tensor=True)
    return embeddings

def load_pdfs():
    """Loads PDFs, extracts text, and generates embeddings."""
    pdf_data = {}
    for pdf_file in os.listdir(PDF_FOLDER):
        if pdf_file.endswith(".pdf"):
            pdf_path = os.path.join(PDF_FOLDER, pdf_file)
            pdf_text = extract_text_with_page_numbers(pdf_path)
            pdf_embeddings = create_embeddings(sentence_model, pdf_text)
            pdf_data[pdf_file] = (pdf_text, pdf_embeddings)
    return pdf_data

cache = Path("in/pdf_emb.pkl")
if cache.exists():
    print("Loading cached embeddings...")
    pdf_data = pickle.loads(cache.read_bytes())
else:
    print("Processing PDFs and storing embeddings...")
    pdf_data = load_pdfs()
    pickle.dump(pdf_data, cache.open("wb"))

def search_answer_in_pdfs(question):
    """Finds the best matching page for a question and provides multiple choices if needed."""
    question_embedding = sentence_model.encode(question, convert_to_tensor=True)
    results = []
    threshold = 0.65  # Minimum confidence level for a valid match
    similarity_gap = 0.05  # How close scores need to be to trigger clarification

    for pdf_name, (pdf_text, pdf_embeddings) in pdf_data.items():
        for page_num, embeddings in pdf_embeddings.items():
            scores = util.pytorch_cos_sim(question_embedding, embeddings)
            max_score, max_idx = scores.max(dim=1)
            score_value = max_score.item()

            if score_value > threshold:
                results.append({
                    "pdf": pdf_name,
                    "page": page_num,
                    "sentence": pdf_text[page_num].split('\n')[max_idx.item()],
                    "score": score_value
                })

    if not results:
        return "No relevant answer found in the PDFs."

    # Sort results by confidence
    results.sort(key=lambda x: x["score"], reverse=True)

    # If multiple results have very close scores, provide choices instead of looping
    top_result = results[0]
    alternative_results = [
        res for res in results[1:3]  # Up to two alternatives
        if abs(top_result["score"] - res["score"]) < similarity_gap
    ]

    if alternative_results:
        options = "\n".join(
            [f"- {res['pdf']} (Page {res['page']}): {res['sentence'][:100]}..." for res in alternative_results]
        )
        return (
            f"Multiple PDFs contain similar information:\n"
            f"1. {top_result['pdf']} (Page {top_result['page']}): {top_result['sentence'][:100]}...\n{options}\n"
            "Please specify which one you'd like more details from."
        )

    return f"Answer found in {top_result['pdf']} (Page {top_result['page']}): {top_result['sentence']}"

def generate_answer(question, context):
    """Generates an AI-powered answer based on extracted context."""
    input_text = f"Question: {question}\nContext: {context}\nAnswer:"
    inputs = gpt2_tokenizer.encode(input_text, return_tensors='pt')
    outputs = gpt2_model.generate(inputs, max_length=150, num_return_sequences=1)
    answer = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer.split("Answer:")[-1].strip()

@app.route('/')
def home():
    with open("templates/chat.html", "rt") as file:
        return file.read()

@app.route('/chat', methods=['POST'])
def chat():
    """Handles user questions and returns relevant answers."""
    data = request.json
    question = data.get('question')
    conversation_history = data.get('history', [])

    if not question:
        return jsonify({"error": "No question provided"}), 400

    retrieved_text = search_answer_in_pdfs(question)

    # If multiple similar PDFs were found, return options instead of generating an answer
    if "Multiple PDFs contain similar information" in retrieved_text:
        return jsonify({"question": question, "answer": retrieved_text})

    # Append the retrieved text to the conversation history
    conversation_history.append({"role": "system", "content": retrieved_text})

    # Generate an answer using the conversation history as context
    context = "\n".join([f"{entry['role']}: {entry['content']}" for entry in conversation_history])
    generated_answer = generate_answer(question, context)

    # Append the generated answer to the conversation history
    conversation_history.append({"role": "assistant", "content": generated_answer})

    return jsonify({
        "question": question,
        "pdf_reference": retrieved_text,
        "answer": generated_answer,
        "history": conversation_history
    })

if __name__ == "__main__":
    print("Starting server...")
    app.run(host='0.0.0.0', port=5000, debug=True)
